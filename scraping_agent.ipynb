{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bug_crawler.github_fetcher import fetch_github_issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dC8whvlAdFUz"
      },
      "outputs": [],
      "source": [
        "from bug_crawler.openai_client import call_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u_OT9Ce9b2fW"
      },
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------\n",
        "# https://github.com/elastic/elasticsearch\n",
        "OWNER = \"elastic\"         # GitHub username or org\n",
        "REPO = \"elasticsearch\"         # Repository name\n",
        "STATE = \"closed\"           # \"open\", \"closed\", or \"all\"\n",
        "PER_PAGE = 50            # Max 100 per page\n",
        "MAX_PAGES = 5           # How many pages to fetch (50*3 = 150 issues)\n",
        "TOKEN = None             # Put a GitHub Personal Access Token here (optional)\n",
        "start_date = \"2024-01-01\"  # ISO 8601 format, e.g. \"2023-10-01\"\n",
        "end_date = None # \"2025-09-30\"    # ISO 8601 format,\n",
        "keywords = None # \"slowdown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching issues using the Search API...\n",
            "An error occurred: GitHub API error: 429 {\n",
            "  \"documentation_url\": \"https://developer.github.com/v3/#abuse-rate-limits\",\n",
            "  \"message\": \"You have triggered an abuse detection mechanism. Please wait a few minutes before you try again.\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Fetching issues using the Search API...\")\n",
        "try:\n",
        "    # Find issues created between Oct 1 and Oct 15, 2025\n",
        "    issues_in_range = fetch_github_issues(\n",
        "        owner=OWNER,\n",
        "        repo=REPO,\n",
        "        state=STATE,      # Search for closed issues\n",
        "        per_page=PER_PAGE,\n",
        "        max_pages=MAX_PAGES,\n",
        "        start_date=start_date,\n",
        "        end_date=end_date,\n",
        "        keywords=keywords,\n",
        "        token=TOKEN,\n",
        "        include_comments=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFound {len(issues_in_range)} issues in the date range.\")\n",
        "    for issue in issues_in_range:\n",
        "        print(f\"  #{issue['number']}: {issue['title']} (Created: {issue['created_at']})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ypN5ieZgAfm",
        "outputId": "9bd0bf7f-5d33-4be0-d78c-25e20ce362f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues = issues_in_range\n",
        "len(issues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Possible performance regression in 9.1.x (TSDS)\n",
            "\n",
            "Description: Hi, guys!\n",
            "\n",
            "On a multi-node cluster, indexing of TSDS (otel-format metrics) is stable on 9.0.x.\n",
            "After upgrading to 9.1.4, the indexing rate drops and the write queue grows, making the cluster unstable.\n",
            "\n",
            "Profiling shows significant time spent in dv.writeField (see pic. 1)\n",
            "Disabling the new 9.1.x optimization with:\n",
            "\n",
            "```ini\n",
            "-Dorg.elasticsearch.index.codec.tsdb.es819.ES819TSDBDocValuesConsumer.enableOptimizedMerge=false\n",
            "```\n",
            "\n",
            "restores normal performance.\n",
            "\n",
            "<img width=\"1879\" height=\"916\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0d519768-cc86-4113-83c9-9e7b6a253a2e\" />\n",
            "\n",
            "<img width=\"890\" height=\"517\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5aecaa33-fc2b-4d70-bc54-20f9b2ea08c5\" />\n",
            "<img width=\"922\" height=\"294\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13c028f7-84ee-4ba9-b519-b8e945775432\" />\n",
            "\n",
            "\n",
            "Can you help figure out the root cause?\n",
            "\n",
            "(using java 21)\n",
            "\n",
            "Comments: Comment by elasticsearchmachine at 2025-09-24T10:57:37Z:\n",
            "Pinging @elastic/es-storage-engine (Team:StorageEngine)\n",
            "\n",
            "Comment by kkrik-es at 2025-09-24T12:00:22Z:\n",
            "Seems similar to https://github.com/elastic/elasticsearch/pull/132597 that should be fixed in v.9.1.2. @martijnvg @dnhatn fyi.\n",
            "\n",
            "Comment by sherman at 2025-09-24T12:16:41Z:\n",
            "@kkrik-es, hi!\n",
            "\n",
            "I’m using 9.1.4 (flamegraph from it), which already includes this patch.\n",
            "\n",
            "Comment by martijnvg at 2025-09-24T12:37:29Z:\n",
            "Thanks @sherman for reporting this. Looks like closing a temporary index output is much slower than expected. We will take a look at get back to you.\n",
            "\n",
            "Looking at the `DISIAccumulator`, looks we close twice, while only once should be needed. I don't think this explains the regression, but that should also be addressed.\n",
            "\n",
            "Comment by martijnvg at 2025-09-24T18:49:14Z:\n",
            "We discussed this and we noticed the following:\n",
            "\n",
            "- The `DISIAccumulator` should only be used during merging, but it is also used here during the flush. This shouldn't happen and needs to be addressed.\n",
            "- The tmp file it tries to create are mmap-ed, perhaps we this shouldn't be done in this case. Which could explain this slowdown under load. (not enough memory for OS to mmap all files and also not the tmp files that are being created) \n",
            "\n",
            "Comment by sherman at 2025-09-25T08:20:55Z:\n",
            "If you mean DISI temp files, I’m not sure they affect memory - they’re too small?\n",
            "\n",
            "```bash\n",
            "[root@hssf86 elasticsearch]# lsof +p 2259807 | grep -i disi | tr -s \" \" | cut -d \" \" -f 7 | awk '{ sum += $1 } END { print sum }'\n",
            "15846\n",
            "[root@hssf86 elasticsearch]# lsof +p 2259807 | grep -i disi | tr -s \" \" | cut -d \" \" -f 7 | awk '{ sum += $1 } END { print sum }'\n",
            "6234\n",
            "[root@hssf86 elasticsearch]# lsof +p 2259807 | grep -i disi | tr -s \" \" | cut -d \" \" -f 7 | awk '{ sum += $1 } END { print sum }'\n",
            "11666\n",
            "[root@hssf86 elasticsearch]# lsof +p 2259807 | grep -i disi | tr -s \" \" | cut -d \" \" -f 7 | awk '{ sum += $1 } END { print sum }'\n",
            "21302\n",
            "```\n",
            "\n",
            "Comment by sherman at 2025-09-25T14:54:17Z:\n",
            "I believe the main problem is not with file writes, but with file reads.\n",
            "From what I see, when we read a file in DISIAccumulator, here:\n",
            "\n",
            "```java\n",
            "try (var addressDataInput = dir.openInput(skipListTempFileName, context)) {\n",
            "    data.copyBytes(addressDataInput, addressDataInput.length());\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "it uses a memory-segment input implementation that relies on a shared arena.\n",
            "On Java 21, the close() method is very expensive (because it leads to deopts of top frames in each thread), and each file here gets its own shared arena. We also observed this in profiling.\n",
            "\n",
            "Comment by dnhatn at 2025-09-25T20:58:53Z:\n",
            "@sherman Thanks so much for providing more detailed insight. I think this is possible, given that writing and reading mmap files happens per field. What is your setting for `index.store.type`?\n",
            "\n",
            "Comment by dnhatn at 2025-09-25T22:35:32Z:\n",
            "And the refresh interval setting `index.refresh_interval`\n",
            "\n",
            "Comment by sherman at 2025-09-26T08:33:28Z:\n",
            "@dnhatn Hi!\n",
            "\n",
            "```\n",
            "\"mode\": \"time_series\",\n",
            "\"refresh_interval\": \"10s\",\n",
            "\n",
            " \"store\": {\n",
            "        \"stats_refresh_interval\": \"10s\",\n",
            "        \"type\": \"\",\n",
            "        \"fs\": {\n",
            "          \"fs_lock\": \"native\"\n",
            "        },\n",
            "```\n",
            "\n",
            "It’s a high-throughput data stream that writes large volumes of metrics in OTEL format.\n",
            "\n",
            "Comment by martijnvg at 2025-09-26T09:52:29Z:\n",
            "@sherman We have merged a change that we think will address the regression that you've reported. When you upgrade to the new bug fix release and the repression doesn't resolve itself, then please feel free to re-open this issue. Thanks for bringing this regression to our attention.\n",
            "\n",
            "> \"refresh_interval\": \"10s\",\n",
            "\n",
            "It is always good to re-evaluate the refresh interval. In the case of metrics, how often is the poll/collection interval and typically refresh should never be lower than the poll/collection interval. Secondly what are the refresh requirements. Is it acceptable to have a 30 second delay until new segments are visible.\n",
            "\n",
            "Also do searches continuously happen during the day? Sometimes it is better to use the defaults. Given that the then periodic refresh doesn't happen all the time and only on the first search the periodic refresh is enabled and stays around until a shard becomes search idle again.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "issue = issues[0]\n",
        "issue_text = f\"Title: {issue['title']}\\n\\nDescription: {issue['body']}\\n\\nComments: {issue.get('comments_thread_text', '')}\"\n",
        "print(issue_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"bug_crawler/prompt_template/filter_application_resource.txt\", \"r\") as f:\n",
        "    FILTER_PROMPT = f.read()\n",
        "\n",
        "\n",
        "response = call_openai(FILTER_PROMPT.format(app_name='Elasticsearch', issue_text=issue_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"application_resource\": \"yes\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
